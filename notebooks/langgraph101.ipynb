{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1fbd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n",
      "I don't have real-time weather data access. To find the current weather in Tokyo, I recommend checking a reliable weather website or a weather app for the most up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response.content)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response.content)\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that can answer questions about the weather.\"),\n",
    "    HumanMessage(content=\"What is the weather in Tokyo?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d037fe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing calculator tool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculate mathematical expressions. Allways use this for any math calculations.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"The result of the expression {expression} is {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error calcuating {expression}: {e}\"\n",
    "    \n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "# Bind tools to the LLM\n",
    "tools = [calculator, search_tool]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"Testing calculator tool\")\n",
    "response = llm_with_tools.invoke(\"What is 25 * 4 + 17?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d6ee521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_11476\\3202056457.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': '',\n",
       " 'additional_kwargs': {'tool_calls': [{'id': 'call_b7KKCqR3dlXE3lmsd8HXyC0R',\n",
       "    'function': {'arguments': '{\"expression\":\"25 * 4 + 17\"}',\n",
       "     'name': 'calculator'},\n",
       "    'type': 'function'}],\n",
       "  'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 20,\n",
       "   'prompt_tokens': 110,\n",
       "   'total_tokens': 130,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "  'system_fingerprint': 'fp_34a54ae93c',\n",
       "  'id': 'chatcmpl-C3KrdmBogAFjSMqW2nTqZTx2jV2gH',\n",
       "  'service_tier': 'default',\n",
       "  'finish_reason': 'tool_calls',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run--29cd8988-2b0e-4310-b551-f0f2e3c41048-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [{'name': 'calculator',\n",
       "   'args': {'expression': '25 * 4 + 17'},\n",
       "   'id': 'call_b7KKCqR3dlXE3lmsd8HXyC0R',\n",
       "   'type': 'tool_call'}],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 110,\n",
       "  'output_tokens': 20,\n",
       "  'total_tokens': 130,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61067ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'calculator',\n",
       "  'args': {'expression': '25 * 4 + 17'},\n",
       "  'id': 'call_b7KKCqR3dlXE3lmsd8HXyC0R',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d79aac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_map = {\n",
    "    \"calculator\": calculator,\n",
    "    \"duckduckgo_search\": DuckDuckGoSearchRun()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01aa75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_calls(response, tool_map):\n",
    "    \"\"\"Executes all tool calls in the LLM response using the tool map.\"\"\"\n",
    "    if not getattr(response, \"tool_calls\", None):\n",
    "        return response\n",
    "    print(f\"Tool calls requested: {len(response.tool_calls)}\")\n",
    "    for tool_call in response.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        args = tool_call[\"args\"]\n",
    "\n",
    "        print(f\"Tool: {tool_name}\")\n",
    "        print(f\"Args: {args}\")\n",
    "        \n",
    "        tool = tool_map.get(tool_name)\n",
    "        if tool:\n",
    "            result = tool.invoke(args)\n",
    "            preview = result[:200] + \"...\" if isinstance(result, str) and len(result) > 200 else result\n",
    "            print(f\"Tool result: {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5819558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is 25 * 4 + 17?\n",
      "Response: \n",
      "Tool calls requested: 1\n",
      "Tool: calculator\n",
      "Args: {'expression': '25 * 4 + 17'}\n",
      "Tool result: The result of the expression 25 * 4 + 17 is 117\n",
      "\n",
      "\n",
      "Query: Search for the latest news abount Artificial Intelligence.\n",
      "Response: \n",
      "Tool calls requested: 1\n",
      "Tool: duckduckgo_search\n",
      "Args: {'query': 'latest news about Artificial Intelligence'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool result: Search the world's information, including webpages, images, videos and more. Google has many special features to help you find exactly what you're looking for. Not your computer? Use a private browsin...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_llm_tool(query):\n",
    "    print(f\"Query: {query}\")\n",
    "    response = llm_with_tools.invoke(query)\n",
    "    print(f\"Response: {getattr(response, 'content', response)}\")\n",
    "    handle_tool_calls(response, tool_map)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "test_llm_tool(\"What is 25 * 4 + 17?\")\n",
    "test_llm_tool(\"Search for the latest news abount Artificial Intelligence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba45b4d",
   "metadata": {},
   "source": [
    "Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b649deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing structured output\n",
      "name='John Smith' age=30 occupation='software engineer' skills=['Python', 'SQL', 'machine learning']\n",
      "Name:  John Smith\n",
      "Age:  30\n",
      "Occupation:  software engineer\n",
      "Skills:  Python, SQL, machine learning\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class PersonInfo(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "    name: str = Field(description=\"The full name of the person.\")\n",
    "    age: int = Field(description=\"The age of the person.\")\n",
    "    occupation: Optional[str] = Field(description=\"The occupation of the person.\")\n",
    "    skills: List[str] = Field(description=\"The skills of the person.\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(PersonInfo)\n",
    "\n",
    "\n",
    "#Test with person info\n",
    "print(\"Testing structured output\")\n",
    "\n",
    "person_prompt = \"\"\"\n",
    "Extract information about the person:\n",
    "\"John Smith is 30 years old and works as a software engineer. He is skilled in Python, SQL, and machine learning.\"\n",
    "\"\"\"\n",
    "\n",
    "response = structured_llm.invoke(person_prompt)\n",
    "print(response)\n",
    "\n",
    "\n",
    "print(\"Name: \", response.name)\n",
    "print(\"Age: \", response.age)\n",
    "print(\"Occupation: \", response.occupation)\n",
    "print(\"Skills: \", \", \".join(response.skills))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6493f753",
   "metadata": {},
   "source": [
    "Langgraph Básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b897c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b69a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot node function created\n"
     ]
    }
   ],
   "source": [
    "def chatbot_node(state: State) -> State:\n",
    "    \"\"\"The main chatbot node that processes messages and generates responses.\"\"\"\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "print(\"Chatbot node function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7bffe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9988879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
    "\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea9c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = graph.get_graph(xray=True).draw_mermaid_png()\n",
    "with open(\"graph.png\", \"wb\") as f:\n",
    "    f.write(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15ead163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(\"graph.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "062dcf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello! My name is Leonardo?\n",
      "AI: Hello, Leonardo! How can I assist you today?\n",
      "User: Do you remember my name?\n",
      "AI: I don’t have the ability to remember personal information or previous interactions. Each conversation is treated independently. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "def test_chatbot(message: str):\n",
    "    \"\"\"Helper function to test our chatbot.\"\"\"\n",
    "    print(f\"User: {message}\")\n",
    "\n",
    "    initial_state = {\"messages\": [HumanMessage(content=message)]}\n",
    "    result = graph.invoke(initial_state)\n",
    "\n",
    "    ai_response = result[\"messages\"][-1].content\n",
    "    print(f\"AI: {ai_response}\")\n",
    "    return result\n",
    "\n",
    "test_cases = [\n",
    "    \"Hello! My name is Leonardo?\",\n",
    "    \"Do you remember my name?\"\n",
    "]\n",
    "\n",
    "for test_message in test_cases:\n",
    "    test_chatbot(test_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02700e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hola', additional_kwargs={}, response_metadata={}, id='710edd64-e86f-419f-835c-2fa30c4d5012'),\n",
       "  AIMessage(content='¡Hola! ¿Cómo puedo ayudarte hoy?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C3LC5hjxHwpg2rLbEZ7VBkLAdOTXO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--0c5cf2a1-36a3-44c2-800a-19294db7167f-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"t1\"}}\n",
    "\n",
    "# Ejecuta al menos una vez para que exista un checkpoint\n",
    "graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hola\"}]}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a33fa38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No checkpointer set",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Lee el último snapshot de estado\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m snap = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(snap.values)                 \u001b[38;5;66;03m# <- estado global (canales)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(snap.metadata.get(\u001b[33m\"\u001b[39m\u001b[33mwrites\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;66;03m# <- últimos writes por nodo (p.ej. {'chatbot': {...}})\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:1219\u001b[39m, in \u001b[36mPregel.get_state\u001b[39m\u001b[34m(self, config, subgraphs)\u001b[39m\n\u001b[32m   1215\u001b[39m checkpointer: BaseCheckpointSaver | \u001b[38;5;28;01mNone\u001b[39;00m = ensure_config(config)[CONF].get(\n\u001b[32m   1216\u001b[39m     CONFIG_KEY_CHECKPOINTER, \u001b[38;5;28mself\u001b[39m.checkpointer\n\u001b[32m   1217\u001b[39m )\n\u001b[32m   1218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m checkpointer:\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo checkpointer set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1222\u001b[39m     checkpoint_ns := config[CONF].get(CONFIG_KEY_CHECKPOINT_NS, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1223\u001b[39m ) \u001b[38;5;129;01mand\u001b[39;00m CONFIG_KEY_CHECKPOINTER \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config[CONF]:\n\u001b[32m   1224\u001b[39m     \u001b[38;5;66;03m# remove task_ids from checkpoint_ns\u001b[39;00m\n\u001b[32m   1225\u001b[39m     recast = recast_checkpoint_ns(checkpoint_ns)\n",
      "\u001b[31mValueError\u001b[39m: No checkpointer set"
     ]
    }
   ],
   "source": [
    "# Lee el último snapshot de estado\n",
    "snap = graph.get_state(config)\n",
    "print(snap.values)                 # <- estado global (canales)\n",
    "print(snap.metadata.get(\"writes\")) # <- últimos writes por nodo (p.ej. {'chatbot': {...}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea072c2f",
   "metadata": {},
   "source": [
    "Adding memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0b92919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b854d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory added to chatbot\n"
     ]
    }
   ],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "graph_with_memory = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Memory added to chatbot\")\n",
    "\n",
    "def chat_with_memory(message: str, thread_id: str):\n",
    "    \"\"\"Chat function with memory\"\"\"\n",
    "    print(f\"\\n User: {message}\")\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    initial_state = {\"messages\": [HumanMessage(content=message)]}\n",
    "    result = graph_with_memory.invoke(initial_state, config)\n",
    "\n",
    "    ai_response = result[\"messages\"][-1].content\n",
    "    print(f\"AI: {ai_response}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d20e2845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing Conversation with Memory:\n",
      "\n",
      " User: Hello! My name is Leonardo\n",
      "AI: Hello, Leonardo! How can I assist you today?\n",
      "\n",
      " User: What is my name?\n",
      "AI: Your name is Leonardo. How can I help you today?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hello! My name is Leonardo', additional_kwargs={}, response_metadata={}, id='26da1b6c-4fda-4761-82a3-f14df4a81ab3'),\n",
       "  AIMessage(content='Hello, Leonardo! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 13, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C3LDUmQf5Zozw5El8zLO7klgpEpbs', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--afe08f04-269f-4348-a42e-e3635cc63270-0', usage_metadata={'input_tokens': 13, 'output_tokens': 11, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='5b9bd6ed-2bcc-4d41-84ae-412e23b90862'),\n",
       "  AIMessage(content='Your name is Leonardo. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 37, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d77b305a29', 'id': 'chatcmpl-C3LDVgEW7v6UziMtB6WDlaOKzjrXR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f48c7847-36e9-4cdb-ac3b-b89fbcc30527-0', usage_metadata={'input_tokens': 37, 'output_tokens': 12, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n Testing Conversation with Memory:\")\n",
    "\n",
    "chat_with_memory(\"Hello! My name is Leonardo\", thread_id=\"thead-1\")\n",
    "chat_with_memory(\"What is my name?\", thread_id=\"thead-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f16c554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={}, next=(), config={'configurable': {'thread_id': 't1'}}, metadata=None, created_at=None, parent_config=None, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_with_memory.get_state(config = {\"configurable\": {\"thread_id\": \"t1\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df24acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for our two-node agent\"\"\"\n",
    "    messages: list[BaseMessage]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "877bc471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"The chatbot node decides whether to use tools or provide direct responses\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "\n",
    "    system_message = \"\"\"\n",
    "    You are a helpful assistant. You have access to web search and calculator tools.\n",
    "\n",
    "    Use the web_search tool when:\n",
    "    - Asked about current events, news, or recent information.\n",
    "    - Need to find specific facts or data.\n",
    "    - Asked about real-time information (weather, stock prices, etc.)\n",
    "\n",
    "    Use the calculator tool when:\n",
    "    - Asked to perform mathematical calculations.\n",
    "    - Need to solve math problems.\n",
    "\n",
    "    For general knowledge questions that don't require real-time data, answer directly.\n",
    "    Be helpful and conversational in your responses.\n",
    "    \"\"\"\n",
    "\n",
    "    all_messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message}\n",
    "    ] + messages\n",
    "\n",
    "    response = llm_with_tools.invoke(all_messages)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "193010aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dedd58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='calculator', description='Calculate mathematical expressions. Allways use this for any math calculations.', args_schema=<class 'langchain_core.utils.pydantic.calculator'>, func=<function calculator at 0x0000028AAE4ED1C0>),\n",
       " DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='auto', source='text'))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f1dd37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool node created\n"
     ]
    }
   ],
   "source": [
    "tool_node = ToolNode(tools)\n",
    "print(\"Tool node created\")\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"tool\", \"end\"]:\n",
    "    \"\"\"\n",
    "    Determine whether to continue to tools or end the conversation\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    return \"end\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "240f2661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "245d65c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled with memory \n",
      "\n"
     ]
    }
   ],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"chatbot\", chatbot_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"chatbot\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Graph compiled with memory \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9febd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the graph\n",
    "from IPython.display import Image, display\n",
    "img = app.get_graph(xray=True).draw_mermaid_png()\n",
    "with open(\"graph.png\", \"wb\") as f:\n",
    "    f.write(img)\n",
    "display(Image(\"graph.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa7d14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "def chat_with_agent(message: str, thread_id: str):\n",
    "    \"\"\"Chat with the two-node agent\"\"\"\n",
    "    print(f\"\\n User: {message}\")\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    messages_to_process = {\n",
    "        \"messages\": [HumanMessage(content=message)]\n",
    "    }\n",
    "\n",
    "    for event in app.stream(messages_to_process, config):\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value:\n",
    "                last_message = value[\"messages\"][-1]\n",
    "                if hasattr(last_message, \"content\") and last_message.content:\n",
    "                    if isinstance(last_message, AIMessage):\n",
    "                        print(f\"AI: {last_message.content}\")\n",
    "                    elif isinstance(last_message, ToolMessage):\n",
    "                        print(f\"Tool: {last_message.content[:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55cec8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " User: I am Leonardo\n",
      "AI: Hello, Leonardo! How can I assist you today?\n",
      "\n",
      " User: Do you remember my name?\n",
      "AI: I don't have the ability to remember personal information or past interactions, including your name. However, I'm here to help you with any questions or information you need! If you'd like, you can share your name with me again.\n",
      "\n",
      " User: What's 15% of 24?\n",
      "Tool: The result of the expression 0.15 * 24 is 3.5999999999999996\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[1].role', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m questions = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI am Leonardo\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDo you remember my name?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCan you tell me more about that?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m ]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43mchat_with_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthread-2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mchat_with_agent\u001b[39m\u001b[34m(message, thread_id)\u001b[39m\n\u001b[32m      6\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: thread_id}}\n\u001b[32m      8\u001b[39m messages_to_process = {\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=message)]\n\u001b[32m     10\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages_to_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2644\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2642\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2643\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2644\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2654\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:640\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    638\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    642\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:384\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    382\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mchatbot_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      6\u001b[39m system_message = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mYou are a helpful assistant. You have access to web search and calculator tools.\u001b[39m\n\u001b[32m      8\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[33mBe helpful and conversational in your responses.\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     22\u001b[39m all_messages = [\n\u001b[32m     23\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_message}\n\u001b[32m     24\u001b[39m ] + messages\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m response = \u001b[43mllm_with_tools\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_messages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5434\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5427\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5428\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5429\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5432\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5433\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5435\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5437\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5438\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:980\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    973\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    977\u001b[39m     **kwargs: Any,\n\u001b[32m    978\u001b[39m ) -> LLMResult:\n\u001b[32m    979\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:799\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    797\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    798\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m         )\n\u001b[32m    806\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    807\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1045\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1043\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1045\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1049\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1131\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1129\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1087\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1044\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1084\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1085\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1086\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1256\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1244\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1251\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1252\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1253\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1254\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1255\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Portfolio\\langgraph_exploration\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[1].role', 'code': None}}",
      "During task with name 'chatbot' and id '915d4622-06ed-832c-6d8b-2506b6cd8c52'"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"I am Leonardo\",\n",
    "    \"Do you remember my name?\",\n",
    "    \"What's 15% of 24?\",\n",
    "    \"What are the latest news about AI?\",\n",
    "    \"Can you tell me more about that?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    chat_with_agent(question, thread_id=\"thread-2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f50f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
